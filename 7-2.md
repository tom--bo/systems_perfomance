[https://www.amazon.co.jp/Systems-Performance-Enterprise-Brendan-Gregg/dp/0133390098:title=Systems Performance: Enterprise and Cloud]を読んでいく。

[http://tombo2.hatenablog.com/entry/2016/12/15/224237:title=前回]の続き。  
7章3節メモリアーキテクチャを2つに分けて、前半のハードウェア部分。

## 7.3 Architecture

この章ではプロセッサとOSを含むハードとソフト両方のメモリアーキテクチャについて紹介する。

### 7.3.1 Hardware

メモリのハードウェアは、メインメモリ、バス、CPUキャッシュ、MMU等がある。

[Main Memory]

最近一般的にメインメモリとして使われているのはDRAM(dynamic random-access memory)である。

[Latency]

メインメモリのレイテンシーは"column address strobe(CAS)"レイテンシとして計測され、これは目的とするアドレス（カラム）にメモリモジュールを送ってから読み出しが可能になるまでの時間である。
これはメモリのタイプによって異なり、DDR3では約10nsである。
I/Oの転送の度に、メモリバスがキャッシュラインを変換するためにこのレイテンシーが複数回発生する。
他にも新しく利用可能になったデータのためのCPUやMMUを含むレイテンシもある。

[Main Memory Architecture]

メインメモリの例として図7.3に一般的な2プロセッサの"uniform memory access(UMA)"システムを示す。
それぞれのCPUは全てのメモリに対して、共通のシステムバスを通じて、統一されたアクセスレイテンシを持っている。

図7.3?

システムが1つのカーネルで運用されているとき、これらは全てのプロセッサで統一して動いている。
また、これはsymmetric multiprocessing(SMP)アーキテクチャでも同様である。

比較として、2プロセッサの"non-uniform memory access(NUMA)"システムの例を図7.4に示す。
これはメモリアーキテクチャの一部となったCPUインターコネクトを使っている。
この仕組に酔って、メインメモリへのアクセス時間はCPUとの位置関係に酔って変化するようになった。

図7.4?

CPU1はDRAM Aとメモリバスを介して直接I/Oを行うことが出来、これを"local memory"と呼ぶ。一方で、CPU1はDRAM Bとは、CPU1とCPUインターコネクト(2ホップ)を介してI/Oを行い、これを"remote memory"と呼ぶ。

[Busses]

個々までで示したように、どれだけ物理的にメインメモリがシステムにつながっていて、実際の実装ではさらなるコントローラやバスが含まれており、以下のうちのいずれかの方法でアクセスされている。

- shared system Busses
  - シングル/マルチプロセッサにおいて、メモリとコントローラ、最後にはメモリバスをブリッジする共有システムバスを介す。
  - これはUMAの例として、図7.3で示したものであり、Intel front-sideバスとして図6.9で示したものでもある。その例のメモリコントローラはNorthbridgeである。
- Direct
  - メモリバスを介して直接紐付けられているメモリを介すシングルプロセッサの仕組み
- Interconnect
  - マルチプロセッサ環境で、それぞれのプロセッサではメモリバスを介して直接接続し、プロセッサ同士はインターコネクトでつなぐ。
  - これは図7.4で示した者で、6章で議論したものである。

[DDR SDRAM]

メモリバスの速度は、どのアーキテクチャであっても、プロセッサとシステムボードによってサポートされている標準メモリインタフェースに影響される。
この一般的な基準は"double data rate synchronous dynamic random access memory(DDR SDRAM)"という。
"double data rate"とは、データ転送を高・低の2値のクロックシグナルで送っていることで、"double-pumped"とも呼ばれる。
また、"synchronous"とはメモリがCPUのクロックと動悸して時間制御されていることである。  
DDR SDRAM基準の例を以下に示す。

表7.1?

DDR4インターフェースの基準は2012年の9月に公開された。
これらは"PC-"につづいてMB/secの転送速度をつけて名前付けされている。
例えばPC-1600等がある。

[Multichannel]

バンド幅を向上するために、複数のメモリバスを平行に使えるようにサポートされている。
一般的なものは、dual-, triple-, quad-チャネルであり、例えばIntel Core i7ではquad-channel DDR3-1600までサポートしており、このメモリバンド幅の最大は51.2GB/sである。


[CPU Caches]

プロセッサはメモリへのアクセス速度を向上するために、オンチップ音ハードウェアキャッシュを持っている。これらは以下のようなレベルわけがされていて、徐々に速度が落ち、容量が増える。

- Level 1 : たいてい命令キャッシュとデータキャッシュを分離している
- Level 2 : 命令キャッシュとデータキャッシュの両方
- Level 3 : 他の大きいレベルのキャッシュ

Level1は仮想メモリアドレスに使われ、Level2は物理メモリアドレスに使われることが多いが、プロセッサによる。  
これらについては6商CPUsでも議論してきた。

[MMU]

memory management unit(MMU)は仮想アドレスから物理アドレスへの変換を行っている。
これらはページごとに行われ、オフセットと伴って直接ページがマッピングされる。
MMUもCPUキャッシュと近いコンテキストで6商で紹介した。

一般的なMMUをCPUキャッシュとメインメモリとともに図7.5に示す。

図7.5?

[Multiple Page Sizes]

最近のプロセッサは複数のプロセッササイズをサポートしており、これによってOSとMMUは4KB, 2MB, 1GBと言った異なるページサイズを扱うことができる。
Solarisベースカーネルでは、複数のページサイズサポートと動的なラージサイズのページ作成の機能を"multiple page size support(MPSS)"と呼んでいる。
Linuxには"huge pages"と呼ばれる一定の特に2MBなどの大きなページサイズを確保する特徴がある。

早期のページ確保はSolarisベースの動的なものより柔軟性は低いが、メモリフラグメンテーションの問題を避けることができる。

[TLB]

図7.5にあるMMUは最初のアドレス変換キャッシュとしてTLBを使っている。その後にはメインメモリにあるページテーブルが続いている。
TLBでマッピングできるエントリーの数は限られているので、より大きなページサイズを使うことで、広範囲のメモリを変換することができるようになる。
これにより、TLBミスを減らし、パフォーアンスを向上させる音ができる。
TLBはさらにそれぞれのページサイズごとにキャッシュを分割することで、より大きいサイズのマッピングを保つ確率を上げている。  
TLBサイズの例として、Intel Core i7プロセッサの提供す4つのTLBを表7.2に示す。

表7.2?

このプロセッサでは1角レベルのTLBしかないが、Intel Core microarchitectureではCPUが複数のメインメモリキャッシュを提供するように2つのレベルのTLBをサポートしている。
TLBの正確な仕組みはプロセッサごとに異なるので、プロセッサベンダーのマニュアルを見て情報を得る必要がある。



